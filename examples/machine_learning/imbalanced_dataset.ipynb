{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced datasets for classification\n",
    "\n",
    "Examples are when the signal is very small relative to the background, like only 2%:\n",
    "* fraud detection\n",
    "* spam filtering\n",
    "* medical diagnosis\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/\n",
    "\n",
    "https://towardsdatascience.com/regression-for-imbalanced-data-with-application-edf93517247c\n",
    "\n",
    "https://github.com/hananahmed1/ML-Regression-for-imbalaced-data/blob/master/UBR_application.R\n",
    "\n",
    "https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3\n",
    "\n",
    "https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
    "\n",
    "Accuracy is wrong as a metric, as predicting no signal gives a 98% accuracy. Need metrics as precision, recall, F1 and confusion matrix.\n",
    "\n",
    "First try to use `predict_proba` instead of `predict` and play with the threshold in the ROC style to find the best operating point.\n",
    "\n",
    "Under-sampling or over-sampling techniques to be applied only on the train dataset. sklearn does it easily with the resample() method:\n",
    "* over-sampling the minority class with replacement to the size of the majority class\n",
    "* under-sampling the majority class without replacement to the size of the minority class\n",
    "\n",
    "SMOTE using `from imblearn.over_sampling import SMOTE`\n",
    "\n",
    "4 methods as above with code examples https://towardsdatascience.com/the-5-most-useful-techniques-to-handle-imbalanced-datasets-6cdba096d55a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling techniques\n",
    "\n",
    "Re-sampling techniques to pre-process the data to reach about equal class A and B and not use weights:\n",
    "\n",
    "### 1. Random under-sampling of the majority class\n",
    "\n",
    "Advantage: the dataset becomes smaller, so it is faster to train. \n",
    "\n",
    "Disadvantages: information that is lost is potentially useful; the \"random\" chosen sample may be biased. Can lead to underfitting and poor generalization to the test set.\n",
    "\n",
    "### 2. Random over-sampling of the minority class\n",
    "\n",
    "Advantage: no information loss\n",
    "Disadvantage: increases likelihood of over-fitting as the minority class appear several times\n",
    "\n",
    "### 3. Cluster-based over-sampling\n",
    "\n",
    "A k-means clustering algorithm is applied separately to both the majority and minority class. Each cluster is over-sampled in such a way as all clusters of the same class have an equal number of instances, and all classes have the same number of instances\n",
    "\n",
    "### 4. Informed over-sampling: Syntethic Minority Over-Sampling Techniques (SMOTE)\n",
    "\n",
    "Advantage: avoids overfitting when exact replicas of the minority class are added to the dataset (as in 2.) Here similar, but not identical (synthetic) datasets are produced and added to the dataset. But like 2. no loss of useful information. \n",
    "\n",
    "Disadvantage: Not consider neighbouring instances in the input phase space that actually belong to other classes. This can lead do noise.\n",
    "\n",
    "### 5. Modified Synthetic Minority Over-Sampling Techniques (MSMOTE)\n",
    "\n",
    "Solves from the problem of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Ensemble Techniques\n",
    "\n",
    "Not change the sample, as above, but change the algorithm.\n",
    "\n",
    "### 1. Bagging techniques\n",
    "\n",
    "Bootstrap Aggregation (bagging) creates N boostrap samples with replacement. Training separately and aggregating the predictions at the end. \n",
    "\n",
    "Advantage: reduces overfitting and variance, improves misclassification rate, in noisy data outperforms boosting.\n",
    "Disadvanage: Works well only if the base classifiers are not bad to start with. Bagging bad classifiers further degrades the performance.\n",
    "\n",
    "### 2. Boosting techniques\n",
    "\n",
    "Start with a weak learner that is further changed based on its performance, giving more weight to events that were not predicted well in the previous sample.\n",
    "\n",
    "#### 2.1 Adaptive Boosting (AdaBoost)\n",
    "\n",
    "First boosting technique created. \n",
    "\n",
    "Advantage: Simple to implement. Not prone to overfitting. Good generalization to any form of overfitting.\n",
    "\n",
    "Disadvantage: Sensitive to noisy data and outliers. \n",
    "\n",
    "#### 2.2 Gradient Boosting \n",
    "\n",
    "It builds the first learner from the training dataset to predict the sample.\n",
    "It calculates a loss function! It uses this loss to build and improved learner for the next stage.\n",
    "At each step the residual of the loss function is calculated using the Gradient Descent Method. \n",
    "The new residual becomes a target variable for the subsequent iteration. \n",
    "\n",
    "Disadvantage: harder to fit than random forests. \n",
    "\n",
    "Need to fine-tune the key main hyper-parameters, or else it leads to over-training: shrinkange parameter, depth of trees and number of trees \n",
    "\n",
    "#### 2.3 Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "A more efficient implementation of the Gradient Boosting method from 2.2. x10 faster as implements parallel processing. \n",
    "\n",
    "Flexible as can define custom optimization functions and evaluation criteria.\n",
    "\n",
    "Has in inbuild mechanism of deadling with missing values.\n",
    "\n",
    "unlike Gradient Boosting that stops splitting as soon as it hits a negative loss, XGBoost splits up to the specified maximum depth, then prunes tree backwards and removes splits with the negative loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ideas\n",
    "\n",
    "Stratified sampling = ... (one suggests to first stratify and then split?)\n",
    "\n",
    "Use weights, like in Random Forest use `class_weight = balanced`\n",
    "\n",
    "https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code example of classification in a highly imbalanced dataset \n",
    "\n",
    "Using the Kaggle dataset on credit card fraud detection, with two classes.\n",
    "\n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = \"/Users/abuzatu/Work/data/finance/credit_card/creditcard.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging level: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_FILE_NAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very unblanced dataset\n",
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(\"Class\", axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.Class\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_test():\n",
    "    #\n",
    "    r = len(y_train)/len(y)\n",
    "    d = y_train.value_counts().to_dict()\n",
    "    ratio = d[1]/d[0]\n",
    "    logging.info(f\"train ratio to total dataset r = {r:.3f} and ratio S/B = {ratio:.6f}:\")\n",
    "    logging.info(f\"\\n{y_train.value_counts()}\")\n",
    "    #\n",
    "    r = len(y_test)/len(y)\n",
    "    d = y_test.value_counts().to_dict()\n",
    "    ratio = d[1]/d[0]\n",
    "    logging.info(f\"test  ratio to total dataset r = {r:.3f} and ratio S/B = {ratio:.6f}:\")\n",
    "    logging.info(f\"\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive way of setting up testing and training sets\n",
    "\n",
    "but the ratio of S/B is not the same in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 0.750 and ratio S/B = 0.001777:\n",
      "INFO:root:\n",
      "0    213226\n",
      "1       379\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.250 and ratio S/B = 0.001590:\n",
      "INFO:root:\n",
      "0    71089\n",
      "1      113\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, # X\n",
    "    y, # y\n",
    "    test_size = 0.25, # 75% in train, 25% in test\n",
    "    random_state = 42)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better way of setting up testing and training sets\n",
    "\n",
    "We have to be careful to have in each sample the same proportion of success/failure rates, or signal/background rates. Therefore it is not enough to divide randomly. But we want the ratio of S/B is the same in both train and test.\n",
    "\n",
    "The trick is to use the option `stratify = df.y, # stratify`\n",
    "\n",
    "Stratification ensures the same ratio of 1 to 0 in the target in the train and test datasets.\n",
    "\n",
    "This is very important in highly unbalanced datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 0.750 and ratio S/B = 0.001730:\n",
      "INFO:root:\n",
      "0    213236\n",
      "1       369\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.250 and ratio S/B = 0.001730:\n",
      "INFO:root:\n",
      "0    71079\n",
      "1      123\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, # X\n",
    "    y, # y\n",
    "    stratify = y, # stratify\n",
    "    test_size = 0.25, # 75% in train, 25% in test\n",
    "    random_state = 42)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "\n",
    "Under-sampling or over-sampling techniques to be applied only on the train dataset. sklearn does it easily with the resample() method:\n",
    "* over-sampling the minority class with replacement to the size of the majority class\n",
    "* under-sampling the majority class without replacement to the size of the minority class\n",
    "\n",
    "It must be done after the sample was split in train and test. And only applied to the train sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 0.750 and ratio S/B = 0.001777:\n",
      "INFO:root:\n",
      "0    213226\n",
      "1       379\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.250 and ratio S/B = 0.001590:\n",
      "INFO:root:\n",
      "0    71089\n",
      "1      113\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# first divide in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, # X\n",
    "    y, # y\n",
    "    test_size = 0.25, # 75% in train, 25% in test\n",
    "    random_state = 42)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83225</th>\n",
       "      <td>59741.0</td>\n",
       "      <td>-1.648591</td>\n",
       "      <td>1.228130</td>\n",
       "      <td>1.370169</td>\n",
       "      <td>-1.735542</td>\n",
       "      <td>-0.029455</td>\n",
       "      <td>-0.484129</td>\n",
       "      <td>0.918645</td>\n",
       "      <td>-0.438750</td>\n",
       "      <td>0.982144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218076</td>\n",
       "      <td>-0.203458</td>\n",
       "      <td>-0.213015</td>\n",
       "      <td>0.011372</td>\n",
       "      <td>-0.304481</td>\n",
       "      <td>0.632063</td>\n",
       "      <td>-0.262968</td>\n",
       "      <td>-0.099863</td>\n",
       "      <td>38.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52800</th>\n",
       "      <td>45648.0</td>\n",
       "      <td>-0.234775</td>\n",
       "      <td>-0.493269</td>\n",
       "      <td>1.236728</td>\n",
       "      <td>-2.338793</td>\n",
       "      <td>-1.176733</td>\n",
       "      <td>0.885733</td>\n",
       "      <td>-1.960981</td>\n",
       "      <td>-2.363412</td>\n",
       "      <td>-2.694774</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.495358</td>\n",
       "      <td>-0.083066</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>-0.347329</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>-0.433294</td>\n",
       "      <td>0.089293</td>\n",
       "      <td>0.212029</td>\n",
       "      <td>61.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21293</th>\n",
       "      <td>31579.0</td>\n",
       "      <td>1.134626</td>\n",
       "      <td>-0.774460</td>\n",
       "      <td>-0.163390</td>\n",
       "      <td>-0.533358</td>\n",
       "      <td>-0.604555</td>\n",
       "      <td>-0.244482</td>\n",
       "      <td>-0.212682</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>-1.136627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.684454</td>\n",
       "      <td>-1.855269</td>\n",
       "      <td>0.171997</td>\n",
       "      <td>-0.387783</td>\n",
       "      <td>-0.062985</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>-0.061178</td>\n",
       "      <td>0.012180</td>\n",
       "      <td>110.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133600</th>\n",
       "      <td>80455.0</td>\n",
       "      <td>0.069514</td>\n",
       "      <td>1.017753</td>\n",
       "      <td>1.033117</td>\n",
       "      <td>1.384376</td>\n",
       "      <td>0.223233</td>\n",
       "      <td>-0.310845</td>\n",
       "      <td>0.597287</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>-0.701533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097023</td>\n",
       "      <td>0.369957</td>\n",
       "      <td>-0.219266</td>\n",
       "      <td>-0.124941</td>\n",
       "      <td>-0.049749</td>\n",
       "      <td>-0.112946</td>\n",
       "      <td>0.114440</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38225</th>\n",
       "      <td>39302.0</td>\n",
       "      <td>-0.199441</td>\n",
       "      <td>0.610092</td>\n",
       "      <td>-0.114437</td>\n",
       "      <td>0.256565</td>\n",
       "      <td>2.290752</td>\n",
       "      <td>4.008475</td>\n",
       "      <td>-0.123530</td>\n",
       "      <td>1.038374</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019733</td>\n",
       "      <td>0.165463</td>\n",
       "      <td>-0.080978</td>\n",
       "      <td>1.020656</td>\n",
       "      <td>-0.300730</td>\n",
       "      <td>-0.269595</td>\n",
       "      <td>0.481769</td>\n",
       "      <td>0.254114</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>75618.0</td>\n",
       "      <td>1.173488</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>0.490512</td>\n",
       "      <td>0.461596</td>\n",
       "      <td>-0.296377</td>\n",
       "      <td>-0.213165</td>\n",
       "      <td>-0.165254</td>\n",
       "      <td>0.119221</td>\n",
       "      <td>-0.114199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186027</td>\n",
       "      <td>-0.574283</td>\n",
       "      <td>0.161405</td>\n",
       "      <td>-0.006140</td>\n",
       "      <td>0.091444</td>\n",
       "      <td>0.109235</td>\n",
       "      <td>-0.020922</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>159000.0</td>\n",
       "      <td>-0.775981</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>-1.142399</td>\n",
       "      <td>-1.241113</td>\n",
       "      <td>1.940358</td>\n",
       "      <td>3.912076</td>\n",
       "      <td>-0.466107</td>\n",
       "      <td>1.360620</td>\n",
       "      <td>0.400697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>-0.019575</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.682820</td>\n",
       "      <td>-1.635109</td>\n",
       "      <td>-0.770941</td>\n",
       "      <td>0.066006</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>89.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>79795.0</td>\n",
       "      <td>-0.146609</td>\n",
       "      <td>0.992946</td>\n",
       "      <td>1.524591</td>\n",
       "      <td>0.485774</td>\n",
       "      <td>0.349308</td>\n",
       "      <td>-0.815198</td>\n",
       "      <td>1.076640</td>\n",
       "      <td>-0.395316</td>\n",
       "      <td>-0.491303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052649</td>\n",
       "      <td>0.354089</td>\n",
       "      <td>-0.291198</td>\n",
       "      <td>0.402849</td>\n",
       "      <td>0.237383</td>\n",
       "      <td>-0.398467</td>\n",
       "      <td>-0.121139</td>\n",
       "      <td>-0.196195</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>87931.0</td>\n",
       "      <td>-2.948638</td>\n",
       "      <td>2.354849</td>\n",
       "      <td>-2.521201</td>\n",
       "      <td>-3.798905</td>\n",
       "      <td>1.866302</td>\n",
       "      <td>2.727695</td>\n",
       "      <td>-0.471769</td>\n",
       "      <td>2.217537</td>\n",
       "      <td>0.580199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332759</td>\n",
       "      <td>-1.047514</td>\n",
       "      <td>0.143326</td>\n",
       "      <td>0.678869</td>\n",
       "      <td>0.319710</td>\n",
       "      <td>0.426309</td>\n",
       "      <td>0.496912</td>\n",
       "      <td>0.335822</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>76381.0</td>\n",
       "      <td>1.233174</td>\n",
       "      <td>-0.784851</td>\n",
       "      <td>0.386784</td>\n",
       "      <td>-0.698559</td>\n",
       "      <td>-1.034018</td>\n",
       "      <td>-0.637028</td>\n",
       "      <td>-0.502369</td>\n",
       "      <td>-0.188057</td>\n",
       "      <td>-0.749637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027634</td>\n",
       "      <td>-0.234522</td>\n",
       "      <td>-0.059544</td>\n",
       "      <td>-0.109073</td>\n",
       "      <td>0.290326</td>\n",
       "      <td>-0.393074</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213605 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "83225    59741.0 -1.648591  1.228130  1.370169 -1.735542 -0.029455 -0.484129   \n",
       "52800    45648.0 -0.234775 -0.493269  1.236728 -2.338793 -1.176733  0.885733   \n",
       "21293    31579.0  1.134626 -0.774460 -0.163390 -0.533358 -0.604555 -0.244482   \n",
       "133600   80455.0  0.069514  1.017753  1.033117  1.384376  0.223233 -0.310845   \n",
       "38225    39302.0 -0.199441  0.610092 -0.114437  0.256565  2.290752  4.008475   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119879   75618.0  1.173488  0.100792  0.490512  0.461596 -0.296377 -0.213165   \n",
       "259178  159000.0 -0.775981  0.144023 -1.142399 -1.241113  1.940358  3.912076   \n",
       "131932   79795.0 -0.146609  0.992946  1.524591  0.485774  0.349308 -0.815198   \n",
       "146867   87931.0 -2.948638  2.354849 -2.521201 -3.798905  1.866302  2.727695   \n",
       "121958   76381.0  1.233174 -0.784851  0.386784 -0.698559 -1.034018 -0.637028   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "83225   0.918645 -0.438750  0.982144  ... -0.218076 -0.203458 -0.213015   \n",
       "52800  -1.960981 -2.363412 -2.694774  ... -1.495358 -0.083066  0.074612   \n",
       "21293  -0.212682  0.040782 -1.136627  ... -0.684454 -1.855269  0.171997   \n",
       "133600  0.597287 -0.127658 -0.701533  ...  0.097023  0.369957 -0.219266   \n",
       "38225  -0.123530  1.038374 -0.075846  ... -0.019733  0.165463 -0.080978   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119879 -0.165254  0.119221 -0.114199  ... -0.186027 -0.574283  0.161405   \n",
       "259178 -0.466107  1.360620  0.400697  ...  0.037078 -0.019575  0.241830   \n",
       "131932  1.076640 -0.395316 -0.491303  ...  0.052649  0.354089 -0.291198   \n",
       "146867 -0.471769  2.217537  0.580199  ... -0.332759 -1.047514  0.143326   \n",
       "121958 -0.502369 -0.188057 -0.749637  ...  0.027634 -0.234522 -0.059544   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "83225   0.011372 -0.304481  0.632063 -0.262968 -0.099863   38.42      0  \n",
       "52800  -0.347329  0.541900 -0.433294  0.089293  0.212029   61.20      0  \n",
       "21293  -0.387783 -0.062985  0.245118 -0.061178  0.012180  110.95      0  \n",
       "133600 -0.124941 -0.049749 -0.112946  0.114440  0.066101   10.00      0  \n",
       "38225   1.020656 -0.300730 -0.269595  0.481769  0.254114   22.00      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "119879 -0.006140  0.091444  0.109235 -0.020922  0.003967    1.98      0  \n",
       "259178  0.682820 -1.635109 -0.770941  0.066006  0.137056   89.23      0  \n",
       "131932  0.402849  0.237383 -0.398467 -0.121139 -0.196195    3.94      0  \n",
       "146867  0.678869  0.319710  0.426309  0.496912  0.335822    1.00      0  \n",
       "121958 -0.109073  0.290326 -0.393074  0.001217  0.038588  113.00      0  \n",
       "\n",
       "[213605 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the training test together\n",
    "df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape=(213605, 31)\n",
      "df_S shape=(379, 31)\n",
      "df_B shape=(213226, 31)\n"
     ]
    }
   ],
   "source": [
    "# separate in minority (signal, S) and majority (background, B) clsses\n",
    "df_S = df_train[df_train.Class == 1]\n",
    "df_B = df_train[df_train.Class == 0]\n",
    "print(f\"df_train shape={df_train.shape}\")\n",
    "print(f\"df_S shape={df_S.shape}\")\n",
    "print(f\"df_B shape={df_B.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape=(213605, 31)\n",
      "df_S shape=(379, 31)\n",
      "df_B shape=(213226, 31)\n",
      "df_S shape=(379, 31)\n",
      "df_B_down_sample shape=(379, 31)\n",
      "df_train_with_B_down_sample shape=(758, 31)\n"
     ]
    }
   ],
   "source": [
    "# under-sample (down-sample) the majority class.\n",
    "# It must be without replacement, to remove an object once selected and not put it back (re-place).\n",
    "# we give the number to be that of equal to the size of the minority sample\n",
    "# the result is that some information was lost and it may be important\n",
    "print(f\"df_train shape={df_train.shape}\")\n",
    "print(f\"df_S shape={df_S.shape}\")\n",
    "print(f\"df_B shape={df_B.shape}\")\n",
    "df_B_down_sample = resample(df_B,\n",
    "                            replace = False,\n",
    "                            n_samples = len(df_S),\n",
    "                            random_state = 42,\n",
    "                           )\n",
    "print(f\"df_S shape={df_S.shape}\")\n",
    "print(f\"df_B_down_sample shape={df_B_down_sample.shape}\")\n",
    "# concatenate the S and the B_down_sample together to create the new df_train\n",
    "df_train_with_B_down_sample = pd.concat([df_B_down_sample, df_S], axis = 0)\n",
    "print(f\"df_train_with_B_down_sample shape={df_train_with_B_down_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape=(213605, 31)\n",
      "df_S shape=(379, 31)\n",
      "df_B shape=(213226, 31)\n",
      "df_S_up_sample shape=(213226, 31)\n",
      "df_B shape=(213226, 31)\n",
      "df_train_with_S_up_sample shape=(426452, 31)\n"
     ]
    }
   ],
   "source": [
    "# over-sample (up-sample) the minority class.\n",
    "# It must be with replacement, as otherwise we run out the samples.\n",
    "# we give the number to be that of equal to the size of the majority sample\n",
    "# the result is that the signal samples appear several times\n",
    "print(f\"df_train shape={df_train.shape}\")\n",
    "print(f\"df_S shape={df_S.shape}\")\n",
    "print(f\"df_B shape={df_B.shape}\")\n",
    "df_S_up_sample = resample(df_S,\n",
    "                          replace = True,\n",
    "                          n_samples = len(df_B),\n",
    "                          random_state = 42,\n",
    "                         )\n",
    "print(f\"df_S_up_sample shape={df_S_up_sample.shape}\")\n",
    "print(f\"df_B shape={df_B.shape}\")\n",
    "# concatenate the S_up_sample and the B together to create the new df_train\n",
    "df_train_with_S_up_sample = pd.concat([df_B, df_S_up_sample], axis = 0)\n",
    "print(f\"df_train_with_S_up_sample shape={df_train_with_S_up_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic samples with SMOTE\n",
    "\n",
    "The signal up-sampling has the problem that the exact same signal sample appear several times, which can lead to overfitting. We generate new samples that are similar, but not identical, to the signal sample.\n",
    "\n",
    "Using the library `imbalance-learn` https://imbalanced-learn.org/stable/\n",
    "\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 0.750 and ratio S/B = 0.001777:\n",
      "INFO:root:\n",
      "0    213226\n",
      "1       379\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.250 and ratio S/B = 0.001590:\n",
      "INFO:root:\n",
      "0    71089\n",
      "1      113\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# first divide in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, # X\n",
    "    y, # y\n",
    "    test_size = 0.25, # 75% in train, 25% in test\n",
    "    random_state = 42)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMOTE(random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up SMOTE\n",
    "sm = SMOTE(random_state = 42)\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 1.497 and ratio S/B = 1.000000:\n",
      "INFO:root:\n",
      "1    213226\n",
      "0    213226\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.250 and ratio S/B = 0.001590:\n",
      "INFO:root:\n",
      "0    71089\n",
      "1      113\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# change the X_train and y_train and indded in train r > 1.0 and S/B is exactly 1.0; while test is not changed.\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without rebalancing data\n",
    "\n",
    "https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train ratio to total dataset r = 0.700 and ratio S/B = 0.001728:\n",
      "INFO:root:\n",
      "0    199020\n",
      "1       344\n",
      "Name: Class, dtype: int64\n",
      "INFO:root:test  ratio to total dataset r = 0.300 and ratio S/B = 0.001735:\n",
      "INFO:root:\n",
      "0    85295\n",
      "1      148\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, # X\n",
    "    y, # y\n",
    "    stratify = y, # stratify\n",
    "    test_size = 0.30, # 70% in train, 30% in test\n",
    "    random_state = 42)\n",
    "check_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(learning_rate=0.01, max_iter=2000,\n",
       "                               max_leaf_nodes=6, n_iter_no_change=15,\n",
       "                               random_state=42, validation_fraction=0.2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model from sklearn\n",
    "gbc = HistGradientBoostingClassifier(\n",
    "    learning_rate = 0.01, # hyper-parameter to tune\n",
    "    max_iter = 2000,\n",
    "    max_leaf_nodes = 6, # hyper-parameter to tune\n",
    "    validation_fraction = 0.2, \n",
    "    n_iter_no_change = 15, # to stop the training early if the performance on a validation subset starts to deteriorate due to overfitting\n",
    "    random_state = 42)\n",
    "gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(learning_rate=0.01, max_iter=2000,\n",
       "                               max_leaf_nodes=6, n_iter_no_change=15,\n",
       "                               random_state=42, validation_fraction=0.2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test\n",
    "# this assumes the 0.5 threshold, >= -0.5 is positive, < 0.5 is negative\n",
    "y_predicted_test = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "def conf_matrix(y, pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0 (Legit)', 'True 1 (Fraud)'], \n",
    "            columns=['Pred 0 (Approve as Legit)', \n",
    "                            'Pred 1 (Deny as Fraud)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0( Approve as Legit)</th>\n",
       "      <th>Pred 1 (Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0 (Legit)</th>\n",
       "      <td>TN = 85284 (TNR = 99.99%)</td>\n",
       "      <td>FP = 11 (FPR = 0.01%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1 (Fraud)</th>\n",
       "      <td>FN = 40 (FNR = 27.03%)</td>\n",
       "      <td>TP = 108 (TPR = 72.97%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0( Approve as Legit)   Pred 1 (Deny as Fraud)\n",
       "True 0 (Legit)  TN = 85284 (TNR = 99.99%)    FP = 11 (FPR = 0.01%)\n",
       "True 1 (Fraud)     FN = 40 (FNR = 27.03%)  TP = 108 (TPR = 72.97%)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix(y_test, y_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907563025210084"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108/(108+11) # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7297297297297297"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108/(108+40) # recall, or True Positive Rate, or TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2702702702702703"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40/(108+40) # False Negative Rate (FNR) - Type I error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998710358168709"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "85284 / (85284+11) # True Negative Rate (TNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012896418312914005"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11 / (85284+11) # False Positive Rate (FPR) - Type 2 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vary the threshold instead of 0.5 and see how the FP and FN change. There is a trade-off. Find a balance.\n",
    "\n",
    "It is like a Receiving Operating Curve (ROC) curve, but changed.\n",
    "\n",
    "We plot the False Positive Rate (FPR, or Type II error) and False Negative Rate (FNR, or Type O error).\n",
    "\n",
    "Both type of errors can not be zero at the same time. There is a trade-off.\n",
    "\n",
    "Business needs will decide the balance, or trade-off version.\n",
    "\n",
    "Use instead of `predict()` the method `predict_proba()`, or predict the probability. If your model does not have this function, use its decision function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probability\n",
    "y_predicted_proba_test = gbc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99994660e-01, 5.33966139e-06],\n",
       "       [9.99935122e-01, 6.48780379e-05],\n",
       "       [9.99788878e-01, 2.11121562e-04],\n",
       "       ...,\n",
       "       [9.99990972e-01, 9.02846156e-06],\n",
       "       [9.99937942e-01, 6.20582725e-05],\n",
       "       [9.99984961e-01, 1.50388011e-05]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85443, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.33966139e-06, 6.48780379e-05, 2.11121562e-04, ...,\n",
       "       9.02846156e-06, 6.20582725e-05, 1.50388011e-05])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note we want to use only one dimension so this\n",
    "y_predicted_proba_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.33966139e-06, 6.48780379e-05, 2.11121562e-04, ...,\n",
       "       9.02846156e-06, 6.20582725e-05, 1.50388011e-05])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so let's incorporate directly \n",
    "y_predicted_proba_test = gbc.predict_proba(X_test)[:, 1]\n",
    "y_predicted_proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predicted_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.99980069, 0.99987104,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00675676, 0.07432432, ..., 1.        , 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99885239e+00, 9.98852390e-01, 9.95476121e-01, ...,\n",
       "       9.61058764e-07, 9.28484349e-07, 5.26525070e-07])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.998852e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.988524e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.954761e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.950777e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.738802e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>1.033531e-06</td>\n",
       "      <td>0.999355</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>1.025276e-06</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>9.610588e-07</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>9.284843e-07</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>5.265251e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Threshold  False Positive Rate  False Negative Rate\n",
       "0     1.998852e+00             0.000000             1.000000\n",
       "1     9.988524e-01             0.000000             0.993243\n",
       "2     9.954761e-01             0.000000             0.925676\n",
       "3     9.950777e-01             0.000000             0.912162\n",
       "4     9.738802e-01             0.000000             0.601351\n",
       "...            ...                  ...                  ...\n",
       "6995  1.033531e-06             0.999355             0.000000\n",
       "6996  1.025276e-06             0.999766             0.000000\n",
       "6997  9.610588e-07             0.999801             0.000000\n",
       "6998  9.284843e-07             0.999871             0.000000\n",
       "6999  5.265251e-07             1.000000             0.000000\n",
       "\n",
       "[7000 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_fpr_tpr_vs_threshold():\n",
    "    df_plot = pd.DataFrame(\n",
    "        {\n",
    "            \"Threshold\" : thresholds, \n",
    "            \"False Positive Rate\" : fpr, \n",
    "            \"False Negative Rate\" : 1.-tpr,\n",
    "        }\n",
    "    )\n",
    "    ax = df_plot.plot(\n",
    "        x = \"Threshold\",\n",
    "        y = [\"False Positive Rate\", \"False Negative Rate\"],\n",
    "        figsize = (10, 6),\n",
    "    )\n",
    "    ax.set_xbound(0, 0.0008)\n",
    "    ax.set_ybound(0, 0.3)\n",
    "    return df_plot\n",
    "df_plot = plot_fpr_tpr_vs_threshold()\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose threshold where the two false rates are about equal, say 0.00035."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.5292e+04, 1.5000e+01, 6.0000e+00, 7.0000e+00, 4.0000e+00,\n",
       "        6.0000e+00, 5.0000e+00, 3.0000e+00, 7.0000e+00, 9.8000e+01]),\n",
       " array([5.26525070e-07, 9.98857128e-02, 1.99770899e-01, 2.99656085e-01,\n",
       "        3.99541272e-01, 4.99426458e-01, 5.99311644e-01, 6.99196831e-01,\n",
       "        7.99082017e-01, 8.98967203e-01, 9.98852390e-01]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_predicted_proba_test, bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.33966139e-06, 6.48780379e-05, 2.11121562e-04, ...,\n",
       "       9.02846156e-06, 6.20582725e-05, 1.50388011e-05])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988523896934093"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_predicted_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if bigger than thrshold, set 1, if smaller, set zero\n",
    "threshold = 0.00035\n",
    "y_predicted_threshold_test = np.where(y_predicted_proba_test > threshold, 1, 0)\n",
    "y_predicted_threshold_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0( Approve as Legit)</th>\n",
       "      <th>Pred 1 (Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0 (Legit)</th>\n",
       "      <td>TN = 80752 (TNR = 94.67%)</td>\n",
       "      <td>FP = 4543 (FPR = 5.33%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1 (Fraud)</th>\n",
       "      <td>FN = 17 (FNR = 11.49%)</td>\n",
       "      <td>TP = 131 (TPR = 88.51%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0( Approve as Legit)   Pred 1 (Deny as Fraud)\n",
       "True 0 (Legit)  TN = 80752 (TNR = 94.67%)  FP = 4543 (FPR = 5.33%)\n",
       "True 1 (Fraud)     FN = 17 (FNR = 11.49%)  TP = 131 (TPR = 88.51%)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate again the confusion matrix\n",
    "conf_matrix(y_test, y_predicted_threshold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0( Approve as Legit)</th>\n",
       "      <th>Pred 1 (Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0 (Legit)</th>\n",
       "      <td>TN = 85284 (TNR = 99.99%)</td>\n",
       "      <td>FP = 11 (FPR = 0.01%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1 (Fraud)</th>\n",
       "      <td>FN = 40 (FNR = 27.03%)</td>\n",
       "      <td>TP = 108 (TPR = 72.97%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0( Approve as Legit)   Pred 1 (Deny as Fraud)\n",
       "True 0 (Legit)  TN = 85284 (TNR = 99.99%)    FP = 11 (FPR = 0.01%)\n",
       "True 1 (Fraud)     FN = 40 (FNR = 27.03%)  TP = 108 (TPR = 72.97%)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remind us the original one\n",
    "conf_matrix(y_test, y_predicted_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
